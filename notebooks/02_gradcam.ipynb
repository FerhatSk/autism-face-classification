{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0586a4f",
   "metadata": {},
   "source": [
    "# Grad-CAM Explainability (Minimal)\n",
    "\n",
    "This notebook:\n",
    "- Loads a trained checkpoint\n",
    "- Runs Grad-CAM on **one** sample image\n",
    "- Saves `assets/gradcam_example.png`\n",
    "\n",
    "Note: Confusion matrix and metrics are produced by `scripts/eval.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422e44c-e03c-49e8-93bc-ca7712034a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install grad-cam opencv-python matplotlib\n",
    "print(\"✅ grad-cam installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14166f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grad-CAM library\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# Repo imports\n",
    "# (Assumes you are running in the repo root or have repo cloned in Colab)\n",
    "from src.models import create_model\n",
    "from src.dataset import get_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EDIT THESE =====\n",
    "MODEL_NAME   = \"resnet50\"  # resnet50 | densenet121 | mobilenet_v3_small\n",
    "WEIGHTS_PATH = \"./saved_models/resnet50_fold1.pth\"\n",
    "\n",
    "# Pick a validation folder (choose one fold val)\n",
    "VAL_DIR      = \"./data/autism_unified_kfold/fold_1/val\"\n",
    "\n",
    "IMG_SIZE     = 224\n",
    "NUM_CLASSES  = 2\n",
    "\n",
    "# Output figure\n",
    "ASSETS_DIR   = \"./assets\"\n",
    "OUT_PATH     = os.path.join(ASSETS_DIR, \"gradcam_example.png\")\n",
    "# ======================\n",
    "\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "print(\"WEIGHTS_PATH:\", WEIGHTS_PATH)\n",
    "print(\"VAL_DIR:\", VAL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab25ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first image under VAL_DIR/*/*.jpg|png|jpeg\n",
    "exts = [\"jpg\", \"jpeg\", \"png\"]\n",
    "candidates = []\n",
    "for e in exts:\n",
    "    candidates += glob.glob(os.path.join(VAL_DIR, \"*\", f\"*.{e}\"))\n",
    "\n",
    "if not candidates:\n",
    "    raise RuntimeError(f\"No images found under: {VAL_DIR}/<class>/*.(jpg|png|jpeg)\")\n",
    "\n",
    "SAMPLE_PATH = candidates[0]\n",
    "print(\"Sample image:\", SAMPLE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "model = create_model(MODEL_NAME, NUM_CLASSES).to(device)\n",
    "state = torch.load(WEIGHTS_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3606ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_layer(model_name: str, model):\n",
    "    name = model_name.lower().strip()\n",
    "    if name == \"resnet50\":\n",
    "        return model.layer4[-1]           # last block\n",
    "    if name == \"densenet121\":\n",
    "        return model.features.denseblock4 # last dense block\n",
    "    if name == \"mobilenet_v3_small\":\n",
    "        return model.features[-1]         # last feature block\n",
    "    raise ValueError(\"Unsupported model_name for Grad-CAM: \" + model_name)\n",
    "\n",
    "target_layer = get_target_layer(MODEL_NAME, model)\n",
    "print(\"Target layer:\", target_layer.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image (RGB)\n",
    "img_bgr = cv2.imread(SAMPLE_PATH)\n",
    "if img_bgr is None:\n",
    "    raise RuntimeError(\"Failed to read image: \" + SAMPLE_PATH)\n",
    "\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Same transforms as test pipeline\n",
    "_, test_tf = get_transforms(IMG_SIZE)\n",
    "tensor = test_tf(image=img_rgb)[\"image\"]          # torch tensor CxHxW\n",
    "input_tensor = tensor.unsqueeze(0).to(device)     # 1xCxHxW\n",
    "\n",
    "print(\"input_tensor:\", tuple(input_tensor.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ffd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    pred_class = int(torch.argmax(logits, dim=1).item())\n",
    "\n",
    "print(\"Predicted class index:\", pred_class)\n",
    "targets = [ClassifierOutputTarget(pred_class)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0]  # HxW\n",
    "\n",
    "print(\"✅ Grad-CAM computed:\", grayscale_cam.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(ASSETS_DIR, exist_ok=True)\n",
    "\n",
    "# show_cam_on_image expects float RGB in [0,1]\n",
    "img_float = img_rgb.astype(np.float32) / 255.0\n",
    "cam_image = show_cam_on_image(img_float, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# Save\n",
    "cam_bgr = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite(OUT_PATH, cam_bgr)\n",
    "\n",
    "print(\"✅ Saved:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,3,1); plt.title(\"Original\"); plt.axis(\"off\"); plt.imshow(img_rgb)\n",
    "plt.subplot(1,3,2); plt.title(\"Grad-CAM\"); plt.axis(\"off\"); plt.imshow(cam_image)\n",
    "plt.subplot(1,3,3); plt.title(\"Heatmap\"); plt.axis(\"off\"); plt.imshow(grayscale_cam, cmap=\"jet\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autism-ml)",
   "language": "python",
   "name": "autism-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
